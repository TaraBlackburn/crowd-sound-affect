{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "import itertools\n",
    "import os, glob\n",
    "import shutil\n",
    "import random\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from model import Inception_v3model, VGG16model\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "images = '/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_training/all_freq'\n",
    "validation = '/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_test/all_freq'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "Inception_model = Inception_v3model(images, validation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "Inception_model.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 30444 images belonging to 3 classes.\n",
      "Found 7616 images belonging to 3 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "Inception_model.model_build()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'callabcks'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-53e2e4aee3dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mInception_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Galvanize/capstones/crowd-sound-affect/src/model.py\u001b[0m in \u001b[0;36mmodel_build\u001b[0;34m(self, learningrate)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     ])\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearningrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_checkpoints/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'callabcks'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "vgg_all = VGG16model(images, validation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "vgg_all.fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 30444 images belonging to 3 classes.\n",
      "Found 7616 images belonging to 3 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "vgg_all.model_fit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "1903/1903 [==============================] - 1497s 785ms/step - loss: 23.8152 - accuracy: 0.8911 - val_loss: 0.1532 - val_accuracy: 0.9610\n",
      "Epoch 2/50\n",
      "1903/1903 [==============================] - 1426s 749ms/step - loss: 0.1050 - accuracy: 0.9681 - val_loss: 0.2344 - val_accuracy: 0.9399\n",
      "Epoch 3/50\n",
      "1903/1903 [==============================] - 1464s 769ms/step - loss: 0.1059 - accuracy: 0.9700 - val_loss: 0.1466 - val_accuracy: 0.9711\n",
      "Epoch 4/50\n",
      "1903/1903 [==============================] - 1395s 733ms/step - loss: 0.1072 - accuracy: 0.9710 - val_loss: 0.1770 - val_accuracy: 0.9643\n",
      "Epoch 5/50\n",
      "1903/1903 [==============================] - 1339s 703ms/step - loss: 0.0675 - accuracy: 0.9826 - val_loss: 0.1647 - val_accuracy: 0.9666\n",
      "Epoch 6/50\n",
      "1903/1903 [==============================] - 1364s 717ms/step - loss: 0.0900 - accuracy: 0.9768 - val_loss: 0.2055 - val_accuracy: 0.9714\n",
      "Epoch 7/50\n",
      "1903/1903 [==============================] - 1409s 740ms/step - loss: 0.0403 - accuracy: 0.9898 - val_loss: 0.3997 - val_accuracy: 0.9497\n",
      "Epoch 8/50\n",
      "1903/1903 [==============================] - 1417s 744ms/step - loss: 0.0763 - accuracy: 0.9853 - val_loss: 0.2414 - val_accuracy: 0.9644\n",
      "Epoch 9/50\n",
      " 756/1903 [==========>...................] - ETA: 13:57 - loss: 0.0352 - accuracy: 0.9913"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-157193f945d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvgg_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Galvanize/capstones/crowd-sound-affect/src/model.py\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m\"\"\"Fits the model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         self.model.fit(x=self.images_train,\n\u001b[0m\u001b[1;32m     48\u001b[0m                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# images are normalized from 0-1. \n",
    "\n",
    "# images_bark = tf.keras.preprocessing.image_dataset_from_directory('../dataset/step4_split_spectrograms/dataset_images_training/bark')\n",
    "# images_dis = tf.keras.preprocessing.image_dataset_from_directory('../dataset/step4_split_spectrograms/dataset_images_training/disapproval_all')\n",
    "# images_neutral+ tf.keras.preprocessing.image_dataset_from_directory('../dataset/step4_split_spectrograms/dataset_images_training/neutral_all')\n",
    "images_bark = '/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_training/bark'\n",
    "validation_bark = '/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_test/bark'\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "images = '/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_training/all_freq'\n",
    "validation = '/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_test/all_freq'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "FULL DATA SET"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "images = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\\\n",
    "    .flow_from_directory(directory=images, target_size=(224,224), batch_size=16)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "test_all = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\\\n",
    "    .flow_from_directory(directory=images, target_size=(224,224), batch_size=16)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 30444 images belonging to 3 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "images"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/pteradox/Galvanize/capstones/crowd-sound-affect/dataset/step4_split_spectrograms/dataset_training/all_freq/*'"
      ]
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\\\n",
    "    .flow_from_directory(directory=validation, target_size=(224,224), batch_size=16)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRAINING"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_bark = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\\\n",
    "    .flow_from_directory(directory=images_bark, target_size=(224,224), batch_size=16)\n",
    "#converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset (normalizing between -1 to 1), without scaling. "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 7611 images belonging to 3 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.DirectoryIterator at 0x7fbf20bb6250>"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "validation_bark = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\\\n",
    "    .flow_from_directory(directory=validation_bark, target_size=(224,224), batch_size=16, shuffle=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1904 images belonging to 3 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "imgs, labels = next(train_bark)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plotImages(imgs)\n",
    "print(labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=3, activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 602115    \n",
      "=================================================================\n",
      "Total params: 621,507\n",
      "Trainable params: 621,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "callback = [tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2), tf.keras.callbacks.ModelCheckpoint(filepath='model_checkpoints/')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Baseline Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(x=train_bark,\n",
    "    steps_per_epoch=len(train_bark),\n",
    "    validation_data=validation_bark,\n",
    "    validation_steps=len(validation_bark),\n",
    "    epochs=50,\n",
    "    verbose=1, callbacks=callback)\n",
    "#batch 32/learning rate .01/categorical_crossentroy, accuracy\n",
    "# 238/238 [==============================] - 242s 1s/step - loss: 0.6378 - accuracy: 0.7752 - val_loss: 0.6460 - val_accuracy: 0.7710"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Improvement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(x=train_bark,\n",
    "    steps_per_epoch=len(train_bark),\n",
    "    validation_data=validation_bark,\n",
    "    validation_steps=len(validation_bark),\n",
    "    epochs=10,\n",
    "    verbose=1, callbacks=[callback])\n",
    "# train_bark = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\\\n",
    "    # .flow_from_directory(directory=images_bark, target_size=(224,224), batch_size=16)\n",
    "    #model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "    # 476/476 [==============================] - 309s 648ms/step - loss: 0.1439 - accuracy: 0.9574 - val_loss: 0.1803 - val_accuracy: 0.9428"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confusion Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "Y_pred = model.predict(validation_bark, 1904// 16+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(validation_bark.classes, y_pred)\n",
    "print(cm)\n",
    "print('Classification Report')\n",
    "target_names = ['Approval', 'Disapproval', 'Neutral']\n",
    "print(classification_report(validation_bark.classes, y_pred, target_names=target_names))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix\n",
      "[[ 353    0    5]\n",
      " [   6   49   23]\n",
      " [  66    9 1393]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Approval       0.83      0.99      0.90       358\n",
      " Disapproval       0.84      0.63      0.72        78\n",
      "     Neutral       0.98      0.95      0.96      1468\n",
      "\n",
      "    accuracy                           0.94      1904\n",
      "   macro avg       0.89      0.85      0.86      1904\n",
      "weighted avg       0.95      0.94      0.94      1904\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "  \n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Approval', 'Disapproval', 'Neutral']); ax.yaxis.set_ticklabels(['Approval', 'Disapproval', 'Neutral']);"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEWCAYAAACZnQc8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyXklEQVR4nO3dd5hURdbH8e9vBpCMgBlQMWcUA+aAq4hrWnNYxbSsCmZ0dXWNq6+uOSuKAVFRjCgiukZwUUBAkKQIKEEEEZEMM3PeP24NNOOE7p6+0z3N+fjcZ7rr3r5V3YOna+rWPSUzwznnXH4oyHYDnHPOZY4HdeecyyMe1J1zLo94UHfOuTziQd055/KIB3XnnMsjHtRdtUlqIOltSQsk9avGec6Q9H4m25YNkgZK6pLtdri1kwf1tYik0yWNkLRI0k8h+OyfgVOfCGwItDSzk9I9iZm9YGaHZ6A9a5B0sCST9EaZ8nah/JMkz3OTpD5VHWdmnc3suTSb61y1eFBfS0i6ArgfuJ0oAG8KPAocm4HTbwZ8a2ZFGThXXOYC+0hqmVDWBfg2UxUo4v9Puazyf4BrAUnNgFuAbmb2upktNrOVZva2mV0VjllH0v2SZoXtfknrhH0HS5oh6UpJc0Iv/5yw72bgBuCU8BfAeWV7tJI2Dz3iOuH52ZKmSFooaaqkMxLKhyS8bl9Jw8OwznBJ+ybs+0TSrZI+D+d5X9J6lXwMK4A3gVPD6wuBU4AXynxWD0iaLul3SV9JOiCUHwH8M+F9fp3QjtskfQ4sAbYIZeeH/Y9Jei3h/HdK+lCSkv39OZcKD+prh32A+sAblRxzHbA3sCvQDtgLuD5h/0ZAM6AVcB7wiKTmZnYjUe//ZTNrbGa9KmuIpEbAg0BnM2sC7AuMLue4FsCAcGxL4F5gQJme9unAOcAGQD2gR2V1A72Bs8LjTsA3wKwyxwwn+gxaAC8C/STVN7P3yrzPdgmvORPoCjQBfihzviuBncMX1gFEn10X8/wcLiYe1NcOLYFfqhgeOQO4xczmmNlc4GaiYFVqZdi/0szeBRYB26bZnhJgJ0kNzOwnMxtXzjF/Br4zs+fNrMjMXgImAkcnHPOMmX1rZkuBV4iCcYXM7H9AC0nbEgX33uUc08fM5oU67wHWoer3+ayZjQuvWVnmfEuIPsd7gT7AxWY2o4rzOZc2D+prh3nAeqXDHxXYhDV7mT+EslXnKPOlsARonGpDzGwx0bDHBcBPkgZI2i6J9pS2qVXC89lptOd5oDtwCOX85SKph6QJYcjnN6K/Tiob1gGYXtlOM/sSmAKI6MvHudh4UF87DAWWA8dVcswsoguepTblj0MTyVoMNEx4vlHiTjMbZGaHARsT9b6fTKI9pW2amWabSj0PXAS8G3rRq4ThkauBk4HmZrYusIAoGANUNGRS6VCKpG5EPf5Z4fzOxcaD+lrAzBYQXcx8RNJxkhpKqiups6T/hMNeAq6XtH644HgD0XBBOkYDB0raNFykvbZ0h6QNJR0bxtaXEw3jlJRzjneBbcI0zDqSTgF2AN5Js00AmNlU4CCiawhlNQGKiGbK1JF0A9A0Yf/PwOapzHCRtA3wb+CvRMMwV0vaNb3WO1c1D+priTA+fAXRxc+5REMG3YlmhEAUeEYAY4CxwMhQlk5dHwAvh3N9xZqBuCC0YxbwK1GAvbCcc8wDjiK60DiPqId7lJn9kk6bypx7iJmV91fIIOA9ommOPwDLWHNopfTGqnmSRlZVTxju6gPcaWZfm9l3RDNoni+dWeRcpskvwjvnXP7wnrpzzuURD+rOOZdHPKg751we8aDunHN5pLKbUbLq3M1P9Cu4Mes9a2i2m+BcRhStmFntXDorf5mSdMypu94WOZu7J2eDunPO1aiS4my3ICM8qDvnHICVdw9c7eNB3TnnAEo8qDvnXN4w76k751weKc7lhbuS50HdOefAL5Q651xe8eEX55zLI36h1Dnn8odfKHXOuXziPXXnnMsjxSurPqYW8KDunHPgF0qdcy6v+PCLc87lEe+pO+dcHvGeunPO5Q8r8QulzjmXP7yn7pxzecTH1J1zLo94Qi/nnMsj3lN3zrk84mPqzjmXR3yRDOecyyPeU3fOufxh5hdKnXMuf3hP3Tnn8kiezH4pyHYDnHMuJ5SUJL9VQdLTkuZI+iah7C5JEyWNkfSGpHUT9l0rabKkSZI6JZQfEcomS7ommbfhQd055yCa/ZLsVrVngSPKlH0A7GRmuwDfAtcCSNoBOBXYMbzmUUmFkgqBR4DOwA7AaeHYSnlQd845iIZfkt2qOpXZZ8CvZcreN7PSb4QvgNbh8bFAXzNbbmZTgcnAXmGbbGZTzGwF0DccWykP6s45BykNv0jqKmlEwtY1xdrOBQaGx62A6Qn7ZoSyisor5RdKnXMOUpr9YmY9gZ7pVCPpOqAIeCGd11fFg3qS6qxTl2tevoW669SloLCQEQOH8tZ9r3Du3d3YtsMOLF24BIBePR5h+vhp7HrYnvzlilMxK6GkqISXbnmG70ZMzPK7qN06HX4w9957C4UFBTz9zEv8565Hst2kvDP52y9YuGgRxcUlFBUVsfc+R2a7STWnBma/SDobOAo41MwsFM8E2iQc1jqUUUl5hTyoJ6lo+UruOv1mli9ZRmGdQq599d+M/WQUAK/c/jxfDfxijeMnfD6W0R8MB6D1dptx4SNXcN2hl9Z4u/NFQUEBDz5wG0cceRozZvzEF0Pf5e133mfChO+y3bS886fDTmLevPnZbkbNizlNgKQjgKuBg8xsScKu/sCLku4FNgG2BoYBAraW1JYomJ8KnF5VPR7UU7B8yTIACusUUlinEKzqYwHWabgOq7+UXTr22nM3vv9+GlOn/gjAK6+8xTFHd/Kg7jIngzcfSXoJOBhYT9IM4Eai2S7rAB9IAvjCzC4ws3GSXgHGEw3LdLNwe6uk7sAgoBB42szGVVW3B/UUqKCAG9+5kw0224iPnh/ElNHfcfBfD+eEHqdxzCUnMeF/Y3n1zj4UrYi+8dt32osTrj6DJi2b8sC5/5fl1tdum7TaiOkzZq16PmPmT+y1525ZbFF+MjMGvvsSZsaTT/bhqV6xDPvmpgwOv5jZaeUU96rk+NuA28opfxd4N5W6Mx7UJbWvbL+Zjcx0nTXFSkq46ciraNC0Id2fuJpW27ThtTtfYMHc36hTrw5d/u8COl9wHG8/+CoAIwcNY+SgYWyz1/b85YpTufuvt2T5HThXuYMO+QuzZs1m/fVb8t7AvkyaNJnBQ77MdrNqhqcJqNA9lewzoGNFO8O0oK4A+7bYjW2bbJHhpmXG0t+XMHHoN+x00G4MerI/AEUrihjS72OO+Nsxfzj+22ETWH/TDWncvAmL5i+s6ebmhVkzZ9Om9SarnrdutTGzZs3OYovyU+lnOnfuPN56ayB77rmrB/VaJuPz1M3skEq2CgN6eG1PM9vDzPbItYDepEVTGjRtCEDddeqx4/7tmP39TJqtv+6qY9ofviczv43GfDfYbKNV5Zvu2JY69ep4QK+G4SNGs9VWbdl88zbUrVuXk08+lrffeT/bzcorDRs2oHHjRqseH/angxg3blKWW1WDzJLfclisY+qSdiK6vbV+aZmZ9Y6zzrg026A5593TnYKCAlQghg/4H19/9BVXvXgjTVo0BYnp46fR+7po6urunfdm3+MPorioiBXLVvB49/uy/A5qt+LiYi697HreHfAihQUFPPvcy4wf/222m5VXNtxwfV7tFw371qlTSN++bzLo/U+y26iaVJQfi2QorlkZkm4kuvq7A9FAf2dgiJmdmMzrz938xNz+OswDvWcNzXYTnMuIohUzVd1zLO1zXdIxp8Ffb6t2fXGJM03AicChwGwzOwdoBzSLsT7nnEtfBrM0ZlOcwy9LzaxEUpGkpsAc1rw7yjnnckeOj5UnK86gPiLkC34S+ApYBPjf+8653JTjPfBkxRbUzeyi8PBxSe8BTc1sTFz1OedctXhQr5yk/kT5f98ys2lx1eOcc5lgxfmx8HScF0rvAfYHxkt6VdKJkupX9SLnnMsKv1BaOTP7FPg0LMnUEfgb8DTQNK46nXMubXmy8HTcNx81AI4GTgHaA8/FWZ9zzqWtxGe/VCqkktwLeA94GPjULE++Cp1z+SfHh1WSFWdPvRdwWmleYOecy2l5cqE0zqD+EdBN0oHh+afA42a2MsY6nXMuPd5Tr9JjQF3g0fD8zFB2fox1OudcenxMvUp7mlm7hOcfSfo6xvqccy59eXLJL8556sWStix9ImkLID8GrZxz+afEkt9yWJw99R7Ax5KmEK2KvRlwToz1Oedc2szH1CsWbjhqB2wNbBuKJ5nZ8jjqc865asuT2S+xDL+EaYynmdlyMxsTNg/ozrnc5cMvVfpc0sPAy8Di0kIzGxljnc45l548GX6J80LprsCOwC1Eyb3uAe6OsT7nnEtfBnvqkp6WNEfSNwllLSR9IOm78LN5KJekByVNljRGUvuE13QJx38nqUsybyPOhF6HxHVu55zLuMxOaXyWKD1K74Sya4APzewOSdeE5/8gWr9567B1ILqfp4OkFsCNwB6AAV9J6m9m8yurOLaeuqSW4dtnpKSvJD0gqWVc9TnnXLVksKduZp8Bv5YpPpbVSQ2fA45LKO9tkS+AdSVtDHQCPjCzX0Mg/wA4oqq64xx+6QvMBU4gWoR6LtH4unPO5RwrKk56k9RV0oiErWsSVWxoZj+Fx7OBDcPjVsD0hONmhLKKyisV54XSjc3s1oTn/5Z0Soz1Oedc+lKY1WJmPYGe6VZlZiYplmk0cfbU35d0qqSCsJ0MDIqxPuecS5+VJL+l5+cwrEL4OSeUzwTaJBzXOpRVVF6pOIP634AXgRVh6wv8XdJCSb/HWK9zzqUu/nnq/YHSGSxdgLcSys8Ks2D2BhaEYZpBwOGSmoeZMoeTRMc4ztkvTeI6t3POZZpl8KYiSS8BBwPrSZpBNIvlDuAVSecBPwAnh8PfBY4EJgNLCOlUzOxXSbcCw8Nxt5hZ2YuvfxD3cnbHEy0+bcBgM3szzvqccy5tRZlLE2Bmp1Ww69ByjjWgWwXneZpobeekxbmc3aPAVsBLoegCSYeZWbmNd865rMrx2/+TFWdPvSOwffgWQtJzwLgY63POufTlSVCP80LpZGDThOdtQplzzuUcM0t6y2Vx9tSbABMkDQvP9wSGS+oPYGbHxFi3c86lJk966nEG9RsSHgs4ADiV6Cqwc87lFg/qlTOzTyXtBpwOnARMBR43s0+Tef3zs4bG1TQXNKpXP9tNWCssL1qZ7Sa4JFhRfqTezXhQl7QNcFrYfiHK9yLP2uicy2n5EdNj6alPBAYDR5nZZABJl8dQj3POZUwmbz7KpjhmvxwP/ES06PSTkg4lGlN3zrnclSfL2WU8qJvZm2Z2KrAd8DFwGbCBpMckHZ7p+pxzLiNKUthyWGzz1M1ssZm9aGZHE2UXG0W0yodzzuUcK7Gkt1wWa+6XUmHVjmrlH3bOuThZUW4H62TVSFB3zrmcl+PDKsnyoO6cc2R63ens8aDunHPgPXXnnMsna2VPPSyp1MbMxsTUHuecyworynYLMqPKoC7pE+CYcOxXwBxJn5vZFTG3zTnnaky+9NSTmafezMx+J7pTtLeZdQD+FG+znHOuZllJ8lsuSyao15G0MdEiqe/E3B7nnMsOU/JbDktmTP0WYBAwxMyGS9oC+C7eZjnnXM3K9R54sqoM6mbWD+iX8HwKcEKcjXLOuZpmJbndA09WhUFd0kNAhffNmtklsbTIOeeyoKQ4c0E9pBs/nyiGjgXOATYG+gItiSadnGlmKyStA/QGdgfmAaeY2bR0666spz4i3ZM651xtk6nhF0mtgEuAHcxsqaRXiJbyPBK4z8z6SnocOA94LPycb2ZbSToVuBM4Jd36KwzqZvZcmYY2NLMl6VbknHO5LMPDL3WABpJWAg2J1pjoSLS8J8BzwE1EQf3Y8BjgVeBhSTKztDKMVTn7RdI+ksYTrWiEpHaSHk2nMuecy1VmyW+SukoakbB1XX0emwncDfxIFMwXEA23/Ga26hanGUCr8LgVMD28tigc3zLd95HM7Jf7gU5A/1Dp15IOTLdC55zLRan01M2swlTi4c77Y4G2wG9EE02OqH4Lk5NUmgAzmy6t8YaL42mOc85lRwYvlP4JmGpmcwEkvQ7sB6wrqU7ojbcGZobjZwJtgBmS6gDNiC6YpiWZm4+mS9oXMEl1JfUAJqRboXPO5SIrUdJbFX4E9pbUUFFv+FBgPNHynieGY7oAb4XH/cNzwv6P0h1Ph+SC+gVAN6Jxn1nAruG5c87lDTMlvVV+HvuS6ILnSKLpjAVEQzX/AK6QNJlozLxXeEkvoGUovwK4pjrvQ9X4QohV3XqtcrNheaRhvfrZbsJaYXnRymw3Ie8tW/ZjtcdOJu/QKemYs9X4QTl7p1Iys1+2kPS2pLmS5kh6K6QKcM65vFFiSnrLZckMv7wIvEJ0N9QmRFdyX4qzUc45V9MyNfySbckE9YZm9ryZFYWtD+B/tzvn8kpJsZLecllluV9ahIcDJV1DlLPAiG5ffbcG2uacczUm7xN6Ed0BZUDpO/17wj4Dro2rUc45V9Nyfaw8WZXlfmlbkw1xzrlsyvWx8mQlM6aOpJ0knSzprNIt7obVJs2aNaVv356MHfspY8Z8wt4dds92k/JGQUEBgz/vz8v9ngTgwIP24bMhbzF02EAee+IuCgsLs9zC2q11640ZNKgvo0Z9yMiR/6Vbt3MBuPHGKxk+fBBffjmQd97pw8Ybb5jllsYvldwvuazKeeqSbgQOBnYgGkvvTLQK0omVva66atM89ad73c+QIV/y9DMvUbduXRo2bMCCBb9nu1lVqg3z1Lt1P5fd2u9MkyaNOfXkrnwzYTDHHPVXvp88jX9efxnTf5zJ8737VX2iLMrleeobbbQBG220AaNHf0Pjxo0YOnQAJ530N2bO/ImFCxcBcNFF57D99ltz8cX/zHJrK5aJeeqjNzsm6Ziz6w/9c7Zbn0xP/USi21xnm9k5QDui3AQOaNq0Cfvv34Gnn4lmea5cubJWBPTaYJNNNqLTEYfQ+7lXAGjRsjkrV6zg+8nTAPj4oyEcc2yN5UnKS7Nnz2H06G8AWLRoMRMnTqZVq41WBXSARo0akqs3KWZSSYmS3nJZMgm9lppZiaQiSU2BOUTJZyok6fjK9pvZ6ym0Mae1bbspv/wyj15P3ccuu+zAyJFjuPyKG1iyZGm2m1br3fGf67nh+jtp3KQRAPN++ZXCOnXYbbedGTVqLMce15lWrTfOcivzx2abtWbXXXdk2LBRANx881WcccYJLFiwkE6d0l6zodbIlwulyfTUR0haF3iSaEbMSGBoFa85upLtqIpelJijuKRkcRJNy746hYXsttvOPPFEb/bcqxOLFy/h6qu7Z7tZtV6nIw5h7tx5q3qRpc49+1Juv/M6PvrkdRYtWkxxsScMzYRGjRry0ktP0KPHzat66TfeeBdbbbU3ffu+yYUXnp3dBtaAfLn5KKXcL5I2B5qa2ZjYWhTUljH1DTdcnyGD32brbfYGYL/99uLqq7pz7HG5fy05l8fUb7ypB6ecdhxFRcXUr78OTZo05u3+g+h6/pWrjunYcX/OOvtkzj4rt5fLzeUxdYA6derwxhvP8MEHn/Lgg0/9YX+bNpvw5pvPsfvuh2WhdcnJxJj6l5scn3TM6TDr9ZyN7JXdfNS+sn1mNjKZCiT9GdiRhLtQzeyWVBqZy37+eS4zZsxim2225Ntvv6djx/2ZMOHbbDer1rv5pru5+aa7Adj/gA5cfMn5dD3/StZbvyW/zJ1HvXr1uOyKv3P3Xb4IV3U98cRdTJw4eY2AvuWWm/P999MAOOqow5k06fssta7m1IpeZBIqG1O/p5J9RrTeXqXC4qoNgUOAp4guug5LpYG1wWWX/4vezz1EvXp1mTL1R84//4psNylvXXrp3+jU+RAKVECvp17gs0+rGgl0ldl33z0544wTGDt2Al9+ORCAG274D2effQrbbLMlJSUl/PjjTC6+OP/vNSwuSWqGd86LNfWupDFmtkvCz8bAQDM7oKrX1pbhl9osl4df8kmuD7/kg0wMvwze6MSkY84Bs1+tfcMvGVI6BWSJpE2Ilmjy6QrOuZxj5GycTkncQf2dMHPmLqJZM0Y0i8Y553JKSZ6MDcQa1M3s1vDwNUnvAPXNbEGcdTrnXDpK8qSnnszKR5L0V0k3hOebStormZNLGiPpn5K2NLPlHtCdc7nKUNJbLkvmcu+jwD7AaeH5QuCRJM9/NFAEvCJpuKQekjZNvZnOORevYpT0lsuSCeodzKwbsAzAzOYD9ZI5uZn9YGb/MbPdgdOBXYCp6TbWOefiUpLClsuSGVNfKamQMDdf0vqk8L4kbUa0WtIpQDFwdRrtdM65WOV6sE5WMj31B4E3gA0k3QYMAW5P5uSSvgyvLQROMrO9zKyym5qccy4rMjmmLmldSa9KmihpgqR9JLWQ9IGk78LP5uFYSXpQ0uRwHbLCu/mTUWVP3cxekPQVUfpdAceZ2YQkz3+WmU2qTgOdc64mZDij7gPAe2Z2oqR6RHfW/xP40MzuCOs+XwP8g2iNiq3D1gF4LPxMSzKzXzYFlgBvA/2BxSlc7Jwt6d7SzIuS7pHkudidczmnBCW9VSbEuAOBXgBmtsLMfgOOBZ4Lhz0HHBceHwv0tsgXwLqS0r5JM5kx9QGsXoC6PtAWmESUpKsqTwPfACeH52cCzwCV5lt3zrmalkoSZ0ldga4JRT3NrGd43BaYCzwjqR1RyvJLgQ3N7KdwzGygdI3AVsD0hHPNCGU/kYZkhl92TnwexnsuSvL8W5rZCQnPb5Y0OvnmOedczShR8uMvIYD3rGB3HaA9cLGZfSnpAaKhlsTXm6RY7mFNOS1ZSLmb7HjPUkn7lz6RtB+r88E451zOsBS2KswAZpjZl+H5q0RB/ufSYZXwc07YP5M1V5NrHcrSUmVPXVJiHtmC0LhZSZ7/AqB3wjj6fKBLSi10zrkakKkpjWY2W9J0SduGiSKHAuPD1gW4I/x8K7ykP9BdUl+iDvOChGGalCUzpt4k4XER0Rj7a1W9KMxtP9PM2oW1TTEzX5HZOZeTMjz75WLghTDzZQpwDlGn+BVJ5wE/sPpa47vAkcBkokkp51Sn4kqDegjMTcysR6onNrPi0qEXD+bOuVyXydv/zWw0sEc5uw4t51gDumWq7sqWs6tjZkVhHDxdoyT1B/oBq1aSNrPXq3FO55zLuAz31LOmsp76MKLx89HVCMz1iRbGSFz6zgAP6s65nJIvaQKSGVNPDMyl89WTCsxmVq2xIeecqyl5skZGpUF9gzDz5RtWB/NSSb1/SVsQ3S67d3jNUOAyM/NMjc65nJIvwy+VzVMvBBqHrUnC49ItGS8CrxCtS7oJ0RBO33Qb65xzcVkbUu/+ZGa3VPP8Dc3s+YTnfSRdVc1zOudcxhXnSU+9sqCeibc4MGQj60s0/HIK8K6kFgBm9msG6nDOuWrL9R54sioL6n+YT5mG0sn1fy9TfipRkN8iA3U451y15X1Qz0Qv2szaVvcczjlXE9aG2S/VJqk+UUbH/Yk+s8HA42a2LM56nXMuVfky+yXWoA70BhYCD4XnpwPPAyfFXK9zzqUk74dfMmQnM9sh4fnHksbHXKdzzqUslUUyclnK+dRTNFLS3qVPJHUARsRcp3POpaxEyW+5LO6e+u7A/yT9GJ5vCkySNJYoOdkuMdfvnHNJ8eGX5BwR8/mdcy4jfPZLEszsBwBJGxAlBist/7HCFwXN6jeKsWUOYMGyxVUf5KptyazB2W6CS0JJnoT1WMfUJR0j6TtgKvApMA0YGGedzjmXjuIUtlwW94XSW4kyNH4bbkQ6FPgi5jqdcy5l+ZLQK+6gvtLM5gEFkgrM7GPKX+LJOeeyyme/JOc3SY2Bz4gWYZ1DwupJzjmXK3xMPTnHEq2OfTnwHvA9cHTMdTrnXMoshS2Xxd1TByAsYD0U2A74vSbqdM65VOT6WHmy4u6pfwbUl9QKeB84E3g25jqdcy5lxVjSWzIkFUoaJemd8LytpC8lTZb0sqR6oXyd8Hxy2L95dd5H3EFdZrYEOB541MxOAnaMuU7nnEtZDLNfLgUmJDy/E7jPzLYC5gPnhfLzgPmh/L5wXNpiD+qS9gHOAAaEssKY63TOuZSVYElvVZHUGvgz8FR4LqAj8Go45DnguPD42PCcsP/QcHxa4g7qlwHXAm+Y2ThJWwAfx1ync86lLJULpZK6ShqRsHUtc7r7gatZ3bFvCfxmZkXh+QygVXjcCpgO0fVHYEE4Pi1xpwn4lOhO0tLnU4BL4qzTOefSkcqFUjPrCfQsb5+ko4A5ZvaVpIMz0LSUxBLUJd1vZpdJeptyZgCZ2TFx1Oucc+lK9gJoEvYDjpF0JFHOq6bAA8C6kuqE3nhrYGY4fibQBpghqQ7QDJiXbuVx9dSfDz/vjun8zjmXUZm6+cjMriUadib01HuY2RmS+gEnAn2BLsBb4SX9w/OhYf9HZpZ2Y2IJ6mb2Vfj5qaT1w+O5cdTlnHOZUAM3Ff0D6Cvp38AooFco7wU8L2ky8CtwanUqiW1MXdJNQHeii7GSVAQ8ZGa3xFWnc86lK440AWb2CfBJeDwF2KucY5aRwXWbY5n9IukKonGlPc2shZk1BzoA+0m6PI46nXOuOjxLY+XOBE4zs6mlBeFb6q/AWTHV6ZxzabMU/stlcQ2/1DWzX8oWmtlcSXVjqtM559KWwdkvWRVXUF+R5j7nnMuKXB9WSVZcQb2dpPKyMYqEtUqdcy5XlKQ/izCnxDWl0fO7OOdqlfwI6TWUT90553Jdvqx85EHdOecg52e1JMuDunPOAUUe1J1zLn94T9055/KIT2l0zrk8Uo3EiDnFg7pzzuGzX5xzLq94mgDnnMsj3lN3zrk84mPqa7mmzZpw/0O3sf0O22BmXNLtWkYMG835fz+T8/52BsXFxXww6BNuvuGubDc1b1zc/TzOPe90JPF0rxd58KGnst2kWuP62+/ls8+H0aL5urzZ53EAHurZm4+GDKVABbRo3ozbrruSDdZvyYLfF/Kv/7uP6TN/Yp169bj1n5ez9Rabs3z5Crp0u4oVK1dSXFTMYYfsT/fzz8zyO8ucfJn9olz9dlqv6Ta52bDg4cfv5Iv/jaBP737UrVuXBg3rs8suO3B5jws57aS/sWLFStZbrwW//PJrtptaoQXLFme7CUnbccdt6dPnUfbd98+sWLGSAe+8QLfu1/D999Oy3bQqLZk1ONtNYMTosTRs0IB/3nr3qqC+aPFiGjdqBECffm/x/dQfufHqi7n74ado2LABF517BlN+mM5t9zxCrwfvwMxYunQZDRs2YGVREWdd2INrLv077XbaPptvDYC6622h6p7j8DZHJB1z3p/+XrXri0tci2TktSZNG7PPvnvQp3c/AFauXMnvCxZy9nmn8cB9PVmxYiVATgf02ma77bZm+LBRLF26jOLiYj4b/AXHHdc5282qNfbYdWeaNW2yRllpQAdYunQZCmHq+2k/0qF9OwC22KwNM3/6mV9+nY8kGjZsAEBRURFFRUVIORvbUlaCJb3lsriWs2tR2RZHnTVps83aMG/efB567A4+Gvwm9z90Gw0bNmDLrdqyz757MOijfvR/tw+7td85203NG+PGTWS//TvQokVzGjSoT+cjOtKm9SbZblat98ATz3LoX85kwPsfrxpK2XarLfjvp58DMHb8JH76eQ4/z4nWvCkuLuaELt048KjT2GfP3dhlx+2y1vZMK7aSpLdcFldP/StgRPhZdhsRU501pk6dQnZptwPP9HqRjgccx+IlS7jkiq7UqVPIus2b0anjSdz4r//w1LP3Z7upeWPixMncfdcjDHz3RQa88wJffz2O4uLc/p+rNrj072fz4RvP8+fDD+HF194G4PwzT2LhosWc0KUbL7zan+223pLCgihUFBYW8tpzj/DhG88zdvy3fDdlWhZbn1n5spxdLEHdzNqa2RbhZ9lti4peJ6mrpBGSRixbsSCOpmXErJmzmTVzNiNHjAHg7TcH0a7djsyaNZsB/d8HYNRXYygxo2XL5tlsal555tm+dNi7Mx0PPYH5vy3gu++mZLtJeeOoww/hv59EvfPGjRrx7+uu4LXnHuH//tWD+b8toHWrjdY4vmmTxuzVfheGfFHr+2irlJglveWy2MfUJTWXtJekA0u3io41s55mtoeZ7VG/XrO4m5a2OXN+YebM2Wy1VVsADjx4HyZNnMzAd/7L/gd2AGDLrTanXt26zJs3P5tNzSvrr98SgDZtNuG44zrzUt83styi2u2H6TNXPf5o8FDabtYagN8XLmLlyui60Gtvv8fuu+5M40aN+HX+b/y+cBEAy5YvZ+jwUbTdrE3NNzwmlsJWGUltJH0sabykcZIuDeUtJH0g6bvws3kol6QHJU2WNEZS++q8j1inNEo6H7gUaA2MBvYGhgId46y3Jlx71a08/tTd1K1Xlx+mzeDii65hyeKlPPjo7Qz+4h1WrlhJ9wv+ke1m5pVXXn6SFi2bU7SyiEsuuY4FC8pbMdGV56ob72D4qDH89tvvHHrcX7novDMZPHQ4036cgQrEJhttwA1XXQzAlB+mc92/70HAlm0345ZrLwNg7rz5XPfvuykuKcFKjE4dD+Dg/Tpk701lWAYvgBYBV5rZSElNgK8kfQCcDXxoZndIuga4BvgH0BnYOmwdgMfCz7TEOqVR0lhgT+ALM9tV0nbA7WZ2fFWvzfUpjfmgNk1prM1yYUpjvsvElMZ9Wh2SdMwZOvPjpOuT9BbwcNgONrOfJG0MfGJm20p6Ijx+KRw/qfS41N5BJO6bj5aZ2TJJSFrHzCZK2jbmOp1zLmVxzGqRtDmwG/AlsGFCoJ4NbBgetwKmJ7xsRijLyaA+Q9K6wJvAB5LmAz/EXKdzzqUslVktkroCXROKeppZzzLHNAZeAy4zs98T5/SbmUmKZTQi1qBuZn8JD2+S9DHQDHgvzjqdcy4dqQxFhwDes6L9kuoSBfQXzOz1UPyzpI0Thl/mhPKZQOIV59ahLC2xzX6RVChpYulzM/vUzPqb2Yq46nTOuXRl6o5SRV3yXsAEM7s3YVd/oEt43AV4K6H8rDALZm9gQbrj6RBjT93MiiVNkrSpmf0YVz3OOZcJGZw0sh9wJjBW0uhQ9k/gDuAVSecRDUOfHPa9CxwJTAaWAOdUp/K4x9SbA+MkDQNWTbUws2Nirtc551JSnKE8jWY2BKhodsyh5RxvQLeMVE78Qf1fMZ/fOecyItfvFE1W3EH9SDNb4w4cSXcCn8Zcr3POpSTXc7okK+40AYeVU+b5Up1zOSdfcr/E0lOXdCFwEbClpDEJu5oA/4ujTuecq4586anHNfzyIjAQ+D+i/AalFpqZrxzhnMs5ud4DT1YsQd3MFgALJJXNaNVYUmOf4uicyzW5vvhFsuK+UDqAKFOlgPpAW2ASsGPM9TrnXEp8+CUJZrbGem4hT/BFcdbpnHPpMO+ppy7kF86fBMzOubyR6wtKJyvuRTKuSHhaALQHZsVZp3POpSPOtSVqUtw99SYJj4uIxthfi7lO55xLmffUk2BmNwNIamhmS+KsyznnqqO4JD/G1GO9o1TSPpLGAxPD83aSHo2zTuecS4el8F8uiztNwP1AJ2AegJl9DRwYc53OOZcyM0t6y2Wxz34xs+mJyzgBxXHX6ZxzqfIx9eRMl7QvYGF5p0uBCTHX6ZxzKcv1Hniy4g7qFwAPEK2MPRN4nwwmg3fOuUzJlwulcc9++QU4I846nHMuE3z4pRKSbqhkt5nZrXHU65xz6fLhl8otLqesEXAe0BLwoO6cyymeercSZnZP6WNJTYgukJ4D9AXuqeh1zjmXLbk+/zxZsY2pS2oBXEE0pv4c0N7M5sdVn3POVYf31Csh6S7geKAnsLOZLYqjHuecy5SSPEm9G9cdpVcCmwDXA7Mk/R62hZJ+j6lO55xLWybvKJV0hKRJkiZLuqbKF2RQXGPqcacfcM65jMrU7BdJhcAjwGHADGC4pP5mNj4jFVTBg69zzhGtu5nsVoW9gMlmNsXMVhBNEDk2lkaXo0ZXPkrFL79/q6qPyi2SuppZz2y3I5/5Zxy/tfUzLloxM+mYI6kr0DWhqGfCZ9YKmJ6wbwZQYyu+eU89s7pWfYirJv+M4+efcRXMrKeZ7ZGw5cyXoAd155zLrJlAm4TnrUNZjfCg7pxzmTUc2FpSW0n1gFOB/jVVec6OqddSOfMnWB7zzzh+/hlXg5kVSeoODAIKgafNbFxN1a98SWLjnHPOh1+ccy6veFB3zrk8slYHdUnHSTJJ22W7LWVJ+kTSHtluR2UkFUsaLWmcpK8lXSmpIOzbQ9KD2W5jdUh6VtKJ2W5HKsK/58QsqT0k3ZTmudaVdFGar50mab10XuuqZ60O6sBpwJDws9okrW0Xnpea2a5mtiPRLdGdgRsBzGyEmV1S0w1aC38HZS0Hjs9QQF0XKDeo++ecu9baoC6pMbA/0cIdp4aygyV9JmlASMbzeELPc5Gk+0Kv9ENJ64fyTyTdL2kEcKmkQyWNkjRW0tOS1gnJffol1H2wpHfC48ckjQjnvbmmP4dMMbM5RDetdFck8T0eFHr0o8Nn00RS4/A5jgyf1bHh2M0lTZT0gqQJkl6V1DDsmybpP+H4YZK2CuXPht/Vl8B/JO0q6QtJYyS9Iam5pO0kDSttb6hnbHh8g6Thkr6R1FNSrbubOUER0eyVy8vukLS+pNfCex0uab9QfpOkHgnHfSNpc+AOYMvwe7sr/E4HS+oPjA/Hvinpq/Dv129aygFrbVAnysXwnpl9C8yTtHso3wu4GNgB2JIohTBEKzeNCL3STwk90qCeme1BlMTnWeAUM9uZaMrohcB/gQ6SGoXjTyHKBwFwXXjtLsBBknbJ+DutIWY2hWgK1wZldvUAupnZrsABwFJgGfAXM2sPHALckxBMtwUeNbPtgd9Zs7e4IHy2DwP3J5S3BvY1syuA3sA/zGwXYCxwo5lNBOpJahuOPwV4OTx+2Mz2NLOdgAbAUdX4GHLBI8AZkpqVKX8AuM/M9gROAJ6q4jzXAN+Hv8auCmXtgUvNbJvw/Fwz2x3YA7hEUsvMvAWXrrU5qJ/G6sDal9VDMMNCIp5i4CWi3jxACauDQJ+EchLKtwWmhi8KiBYHOdDMioD3gKPDn61/Bt4Kx5wsaSQwCtiR6Msk33wO3CvpEmDd8HkIuF3SGKIvvVbAhuH46Wb2eXhc9rN+KeHnPgnl/cysOASydc3s01D+HHBgePwKUTCHNYP6IZK+DD33jkS/h1rLzH4n+mIrO/z1J+BhSaOJboZpGv5iTcUwM5ua8PwSSV8DXxDdRbl1eq12mbJWjospWpWpI7CzJCPqXRowgD8mYatoIn9ieXlrspbVF+gO/ErU418Yeo09gD3NbL6kZ4H6Sb+RHCNpC6AYmANsX1puZndIGgAcCXwuqROwN7A+sLuZrZQ0jdXvvbLfQUWPk/kdvAz0k/R61Cz7TlJ94FFgDzObruiiYq39HSS4HxgJPJNQVgDsbWbLEg+UVMSaHbzK3v+qz1nSwURfFPuY2RJJn1TxWlcD1tae+onA82a2mZltbmZtgKlEQwN7Kbq9t4CoNzckvKYgvA7g9ITyRJOAzUvHeoEziYZqCD/bA39j9V8ITYn+J1kgaUOiC421UrjG8DjRUIaV2belmY01szuJbqHeDmgGzAkB/RBgs4SXbCqptBde9rNO7GkPLdsOM1sAzJd0QCha9Tsws++JvnT+xepeemkQ+iX0WmvVbJeKmNmvRH+ZnJdQ/D7R0CIAknYND6cR/dtEUnugdIhqIdCkkmqaAfNDQN+O6IvaZdla2VMnGmq5s0zZa0Tj38OJxmu3Aj4G3gj7FxMF/OuJeqKnlHk9ZrZM0jlEvcE64VyPh33F4cLh2UCXUPa1pFHARKJUnZ+XPWeOaxD+lK9LdIHueeDeco67LATuEmAcMJAoWLwdhjxGEH0GpSYB3SQ9TXRB7rGEfc3DkM1yKp611AV4PFxgnUK06Hmpl4G7CIHLzH6T9CTwDTCb6HeWL+4h+uuw1CXAI+HzqwN8BlxA9G//LEnjgC+BbwHMbJ6kzyV9Q/Q7G1Dm/O8BF0iaQPQ7+yLON+OS42kCEoQ/J3uY2R8ulElaZGapjj+6FIVZF++Ei5Zl900jGib5pabb5VxtsbYOvzjnXF7ynrpzzuUR76k751we8aDunHN5xIO6c87lEQ/q7g+0OvviN5L6leZeSfNcqzIdSnpKUoV3zIbcIvumUUe5GQErKi9zzKIU61ojT4pzucaDuitPafbFnYAVRHOZV1GaGfrM7HwzG1/JIQcDKQd159xqHtRdVQYDW5XN0CepMGTuG64oG+LfARR5WFGWy/+SkNxLCTniFWWuHKkoD/uHYX76BcDl4a+EA1RxVsGWkt4PmQGfIsojUylVkk1Q5Wff3FLSe+E1g1VOzn1Jl0gaH95/37L7ncuGtfWOUpeE0CPvTHTnIES3ku9kZlNDYFxgZntKWocop8v7wG5Eic12IErQNR54usx51weeJEp2NlVSCzP7VdLjwCIzuzsc9yJRVsEhkjYlWsh3e6IMmUPM7BZJf2bNW+Ercm6oowEwXNJrZjaP1dk3L5d0Qzh3d6L0tReE/DAdiPLDdCxzzmuAtma2XNK6yXymzsXNg7orT+nt/xD11HsRDYskZug7HNhFq1cGakaUoe9A4KWQ5XKWpI/KOf/ewGel5wp5SsrzJ2AHrU5vXppV8EBCSmQzGyBpfhLv6RJJfwmPS7MJzuOP2TdfD3XsS5TuofT165RzzjHAC5LeBN5Mog3Oxc6DuivP0pD7fJUQ3BIzIQq42MwGlTnuyAy2o6KsgimdRKllE7RQ729lP4Ny/JnoC+Zo4DpJO4e0ws5ljY+pu3QNAi6UVBdA0jaKFgH5DDgljLlvTLQARllfAAcqLFihKBUy/DErYEVZBT8jyt6IpM5A8yraWlk2wT9k3wz5yKdKOinUIUntEk+oKItnGzP7GPhHqMNzA7ms86Du0vUU0Xj5yJDF7wmiv/zeAL4L+3pTfnrcuURL372uaIGF0uGPt4G/lF4oJcoquEe4EDme1bNwbib6UhhHNAzzYxVtfQ+ooyib4B2smU2wNPvmN0Rj5reE8jOA80L7xhGtlJWoEOijKMvkKOBBM/utinY4FzvP/eKcc3nEe+rOOZdHPKg751we8aDunHN5xIO6c87lEQ/qzjmXRzyoO+dcHvGg7pxzeeT/AcoYJ8Sp+2Y9AAAAAElFTkSuQmCC"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inception V3 Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "train_bark_incep = ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)\\\n",
    "    .flow_from_directory(directory=images_bark, target_size=(224,224), batch_size=16)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "source": [
    "valid_bark_incep = ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)\\\n",
    "    .flow_from_directory(directory=validation_bark, target_size=(224,224), batch_size=16)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1904 images belonging to 3 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "model_incep = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=3, activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "model_incep.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "model_incep.fit(x=train_bark_incep,\n",
    "    steps_per_epoch=len(train_bark_incep),\n",
    "    validation_data=valid_bark_incep,\n",
    "    validation_steps=len(valid_bark_incep),\n",
    "    epochs=50,\n",
    "    verbose=1, callbacks=[callback])\n",
    "\n",
    "# 476/476 [==============================] - 258s 542ms/step - loss: 9.4135e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9984"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "476/476 [==============================] - 258s 541ms/step - loss: 0.0864 - accuracy: 0.9788 - val_loss: 0.0582 - val_accuracy: 0.9790\n",
      "Epoch 2/50\n",
      "476/476 [==============================] - 248s 520ms/step - loss: 0.0449 - accuracy: 0.9875 - val_loss: 0.0378 - val_accuracy: 0.9900\n",
      "Epoch 3/50\n",
      "476/476 [==============================] - 253s 530ms/step - loss: 0.0206 - accuracy: 0.9936 - val_loss: 0.0516 - val_accuracy: 0.9795\n",
      "Epoch 4/50\n",
      "476/476 [==============================] - 247s 519ms/step - loss: 0.0189 - accuracy: 0.9932 - val_loss: 0.0178 - val_accuracy: 0.9921\n",
      "Epoch 5/50\n",
      "476/476 [==============================] - 241s 506ms/step - loss: 0.0235 - accuracy: 0.9933 - val_loss: 0.0261 - val_accuracy: 0.9911\n",
      "Epoch 6/50\n",
      "476/476 [==============================] - 256s 538ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0049 - val_accuracy: 0.9989\n",
      "Epoch 7/50\n",
      "476/476 [==============================] - 253s 531ms/step - loss: 0.0071 - accuracy: 0.9984 - val_loss: 0.0256 - val_accuracy: 0.9942\n",
      "Epoch 8/50\n",
      "476/476 [==============================] - 262s 551ms/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 0.0037 - val_accuracy: 0.9984\n",
      "Epoch 9/50\n",
      "476/476 [==============================] - 259s 544ms/step - loss: 4.0307e-04 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9968\n",
      "Epoch 10/50\n",
      "476/476 [==============================] - 267s 561ms/step - loss: 1.1586e-04 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 0.9989\n",
      "Epoch 11/50\n",
      "476/476 [==============================] - 258s 542ms/step - loss: 9.4135e-05 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 0.9984\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbeea4d4040>"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f19acf6f74a7eb80785c6028b5d0046357fb08ccf452a8c71704de342991f470"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}